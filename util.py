import math
from typing import List, Tuple, Dict, Set, Union
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
import numpy as np
import math
import pickle
import sys
import time
from collections import namedtuple

from vocab import Vocab, VocabEntry

def input_transpose(sents, pad_token):
    """
    This function transforms a list of sentences of shape (batch_size, token_num) into 
    a list of shape (token_num, batch_size). You may find this function useful if you
    use pytorch
    """
    max_len = max(len(s) for s in sents)
    batch_size = len(sents)

    sents_t = []
    for i in range(max_len):
        sents_t.append([sents[k][i] if len(sents[k]) > i else pad_token for k in range(batch_size)])

    return sents_t


def read_corpus(file_path, source):
    data = []
    # for line in open(file_path):
    for line in open(file_path, encoding = 'utf8'):
        sent = line.strip().split(' ')
        # only append <s> and </s> to the target sentence
        # if source == 'tgt':
        #     sent = ['<s>'] + sent + ['</s>']
        data.append(sent)

    return data


def batch_iter(data, batch_size, shuffle=False):
    """
    Given a list of examples, shuffle and slice them into mini-batches
    """
    batch_num = math.ceil(len(data) / batch_size)
    index_array = list(range(len(data)))

    if shuffle:
        np.random.shuffle(index_array)

    for i in range(batch_num):
        indices = index_array[i * batch_size: (i + 1) * batch_size]
        examples = [data[idx] for idx in indices]

        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)
        src_sents = [e[0] for e in examples]
        tgt_sents = [e[1] for e in examples]
        tgt_sents_mono = [e[2] for e in examples]

        yield src_sents, tgt_sents, tgt_sents_mono


def beam_search(model, test_data_src, beam_size: int, max_ratio: int):
    was_training = model.training

    hypotheses = []
    for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):
        example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_ratio=max_ratio)

        hypotheses.append(example_hyps)

    return hypotheses


# def decode(args: Dict[str, str]):
#     """
#     performs decoding on a test set, and save the best-scoring decoding results. 
#     If the target gold-standard sentences are given, the function also computes
#     corpus-level BLEU score.
#     """
#     test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')
#     if args['TEST_TARGET_FILE']:
#         test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')

#     print(f"load model from {args['MODEL_PATH']}", file=sys.stderr)
#     model = NMT.load(args['MODEL_PATH'])

#     hypotheses = beam_search(model, test_data_src,
#                              beam_size=int(args['--beam-size']),
#                              max_decoding_time_step=int(args['--max-decoding-time-step']))

#     if args['TEST_TARGET_FILE']:
#         top_hypotheses = [hyps[0] for hyps in hypotheses]
#         bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)
#         print(f'Corpus BLEU: {bleu_score}', file=sys.stderr)

#     with open(args['OUTPUT_FILE'], 'w') as f:
#         for src_sent, hyps in zip(test_data_src, hypotheses):
#             top_hyp = hyps[0]
#             hyp_sent = ' '.join(top_hyp.value)
#             f.write(hyp_sent + '\n')

def compute_corpus_level_bleu_score(references, hypotheses) -> float:
    """
    Given decoding results and reference sentences, compute corpus-level BLEU score

    Args:
        references: a list of gold-standard reference target sentences
        hypotheses: a list of hypotheses, one for each reference

    Returns:
        bleu_score: corpus-level BLEU score
    """
    if references[0][0] == '<s>':
        references = [ref[1:-1] for ref in references]

    bleu_score = corpus_bleu([[ref] for ref in references],
                             [hyp for hyp in hypotheses])

    return bleu_score
